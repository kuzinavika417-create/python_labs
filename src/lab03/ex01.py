def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if not text:
        return ""
    for i in "\t\r\n\v\f":
        text = text.replace(i, " ")
    while "  " in text:
        text = text.replace("  ", " ")
    text = text.strip()
    if yo2e:
        text = text.replace("—ë", "–µ").replace("–Å", "–ï")
    if casefold:
        text.casefold()
    return text


def tokenize(text: str) -> list[str]:
    text = normalize(text, casefold=False, yo2e=False)
    words = []
    results = []
    for i in text:
        if i.isalnum() or i == "_" or i == "-" in words:
            words.append(i)
        elif words:
            results.append("".join(words))
            words = []
    if words:
        results.append("".join(words))
    return results


test_normalize = "–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"
test1_normalize = "—ë–∂–∏–∫, –Å–ª–∫–∞"
test2_normalize = "Hello\r\nWorld"
test3_normalize = "  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "
test_tokenize = "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
test2_tokenize = "hello,world!!!"
test3_tokenize = "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"
test4_tokenize = "2025 –≥–æ–¥"
test5_tokenize = "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"
print("—Ç–µ—Å—Ç –∫–µ–π—Å—ã –¥–ª—è normalize:")
print(f"{repr(test_normalize)}, {repr(normalize(test_normalize).casefold())}")
print(f"{repr(test1_normalize)}, {normalize(test1_normalize)}")
print(f"{repr(test2_normalize)}, {normalize(test2_normalize)}")
print(f"{repr(test3_normalize)}, {normalize(test3_normalize)}")

print("—Ç–µ—Å—Ç –∫–µ–π—Å—ã –¥–ª—è tokenize")
print(f"{repr(test_tokenize)}, {repr(tokenize(test_tokenize))}")
print(f"{repr(test2_tokenize)}, {repr(tokenize(test2_tokenize))}")
print(f"{repr(test3_tokenize)}, {repr(tokenize(test3_tokenize))}")
print(f"{repr(test4_tokenize)}, {repr(tokenize(test4_tokenize))}")
print(f"{repr(test5_tokenize)}, {repr(tokenize(test5_tokenize))}")
